{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c01dcf",
   "metadata": {},
   "source": [
    "# üß† NLP 102 ‚Äì Word Embeddings & Visualizing Meaning\n",
    "\n",
    "## Featuring The Hitchhiker‚Äôs Guide to the Galaxy\n",
    "### ‚è±Ô∏è Duration: ~30 minutes\n",
    "### üõ†Ô∏è Requirements: Python 3, Jupyter Notebook or any Python IDE, nltk, gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba0057",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Overview\n",
    "\n",
    "Traditional methods like Bag-of-Words or TF-IDF ignore word meaning and context. That‚Äôs where embeddings shine.\n",
    "Embeddings represent words as dense vectors in a multi-dimensional space where semantic similarity = spatial closeness.\n",
    "\n",
    "In this notebook, you will:\n",
    "- Understand what word embeddings are and why they are powerful\n",
    "- Learn how to train a simple Word2Vec model using gensim\n",
    "- Use UMAP to reduce the dimensionality of word vectors\n",
    "- Create an interactive plot to explore word relationships visually\n",
    "\n",
    "üß© By the end, you'll be able to see how similar words cluster together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a2cd3",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f1583-76fb-4eea-9bd1-b7c118f95bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Time is an illusion. Lunchtime doubly so.\",\n",
    "    \"The ships hung in the sky in much the same way that bricks don‚Äôt.\",\n",
    "    \"The Hitchhiker‚Äôs Guide to the Galaxy is a wholly remarkable book.\",\n",
    "    \"The Answer to the Great Question... of Life, the Universe and Everything... is... Forty-two.\",\n",
    "    \"Don‚Äôt Panic.\",\n",
    "    \"So long, and thanks for all the fish.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dab795",
   "metadata": {},
   "source": [
    "## üß™ Exercise 0: Prepare your data\n",
    "\n",
    "**Goal:** Tokenize the corpus\n",
    "\n",
    "**Optional:** Load the whole book from disk and use it as corpus\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`word_tokenize` from `nltk.tokenize`\n",
    "\n",
    "`simple_preprocess` from `gensim.utils`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Preprocess the data\n",
    "- Create a corpus\n",
    "- Optional: Load book from disk\n",
    "- Optional: Split sentences\n",
    "- Optional: Create corpus from whole book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b998d61-8a3e-46ce-9fba-c37b68bade66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c25b8-5643-4db9-8fc0-8822a52e2879",
   "metadata": {},
   "source": [
    "## üß™ Exercise 1: Create Model and explore word similarities\n",
    "\n",
    "**Goal:** Train a Word2Vec Model \n",
    "\n",
    "**Optional:** Explore different parameters\n",
    "\n",
    "**Super Optional:** Test a word not in the vocabulary\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`Word2Vec` from `gensim.models`\n",
    "\n",
    "`wv.most_similar` from your trained model\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Create an instance of `Word2Vec`\n",
    "- Use the tokenized corpus as data\n",
    "- Test some word similarities\n",
    "- Optional: Make one with `skip-gram` and one with `CBOW`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcddb9f8-0093-4456-a444-1004ecd3b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6436b3",
   "metadata": {},
   "source": [
    "## üß™ Exercise 2: Visualize with UMAP\n",
    "\n",
    "**UMAP (Uniform Manifold Approximation and Projection)** is a dimensionality reduction technique that preserves both local and global structure in data, making it great for visualizing high-dimensional embeddings. It works by modeling the data as a graph and optimizing a low-dimensional representation that maintains the original relationships as closely as possible.\n",
    "\n",
    "**Goal:** Reduce the word2vec vectors to two dimensions and visualize them.\n",
    "\n",
    "**Optional:** Use t-SNE as alternative\n",
    "\n",
    "**Super Optional:** Experiment with different parameters\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`UMAP` from `umap`\n",
    "\n",
    "`array` from `numpy`\n",
    "\n",
    "Optional: `TSNE` from `sklearn.manifold`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Extract all words and vectors\n",
    "- Reduce dimensions\n",
    "- Plot the result\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "Plot of the reduced word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc5c91-4cf2-40fd-852b-bd3fa761ec51",
   "metadata": {},
   "source": [
    "## üß™ Exercise 3: Interactive plot with plotly\n",
    "\n",
    "**Goal:** Create a plot where you can interactively explore the UMAP data\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`scatter` from `plotly`\n",
    "\n",
    "`dataframe` from `pandas`\n",
    "\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Prepare a dataframe\n",
    "- Make an interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb89f8-a45d-41f0-8e70-08cd38fb19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7a842-8cc2-4b16-bef6-8690640aebf2",
   "metadata": {},
   "source": [
    "## üß™ Exercise 4: Use pre-trained Word2Vec\n",
    "\n",
    "**Goal:** Load a pre-trained Word2Vec model and explore what is different\n",
    "\n",
    "**Optional:** Load two pre-trained Word2Vec models and explore how they differ\n",
    "\n",
    "**Super Optional:** Visualize some word from a pretrained Word2Vec model\n",
    "\n",
    "**Super Super Optiona:** Visualize some words from two pretrained models in the same plot\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`load` from `gensim.downloader`\n",
    "\n",
    "`info` from `gensim.downloader` (Use this to list available models)\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Load a pretrained model\n",
    "- Create a list of words you want to test\n",
    "- Calculate the 3 most similar\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "üîé Word: computer\n",
    "3 most similar: [('computers', 0.916504442691803), ('software', 0.8814992904663086), ('technology', 0.852556049823761)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6824223-e923-4b87-a15d-78cbd05e7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
