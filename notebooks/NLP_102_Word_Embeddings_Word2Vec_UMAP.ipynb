{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c01dcf",
   "metadata": {},
   "source": [
    "# üß† NLP 102 ‚Äì Word Embeddings & Visualizing Meaning\n",
    "\n",
    "## Featuring The Hitchhiker‚Äôs Guide to the Galaxy\n",
    "### ‚è±Ô∏è Duration: ~60 minutes\n",
    "### üõ†Ô∏è Requirements: Python 3, Jupyter Notebook or any Python IDE, nltk, gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba0057",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Overview\n",
    "\n",
    "Traditional methods like Bag-of-Words or TF-IDF ignore word meaning and context. That‚Äôs where embeddings shine.\n",
    "Embeddings represent words as dense vectors in a multi-dimensional space where semantic similarity = spatial closeness.\n",
    "\n",
    "In this notebook, you will:\n",
    "- Understand what word embeddings are and why they are powerful\n",
    "- Learn how to train a simple Word2Vec model using gensim\n",
    "- Use UMAP to reduce the dimensionality of word vectors\n",
    "- Create an interactive plot to explore word relationships visually\n",
    "\n",
    "üß© By the end, you'll be able to see how similar words cluster together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a2cd3",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f1583-76fb-4eea-9bd1-b7c118f95bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Time is an illusion. Lunchtime doubly so.\",\n",
    "    \"The ships hung in the sky in much the same way that bricks don‚Äôt.\",\n",
    "    \"The Hitchhiker‚Äôs Guide to the Galaxy is a wholly remarkable book.\",\n",
    "    \"The Answer to the Great Question... of Life, the Universe and Everything... is... Forty-two.\",\n",
    "    \"Don‚Äôt Panic.\",\n",
    "    \"So long, and thanks for all the fish.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dab795",
   "metadata": {},
   "source": [
    "## üß™ Exercise 0: Prepare your data\n",
    "\n",
    "**Goal:** Tokenize the corpus\n",
    "\n",
    "**Optional:** Load the whole book from disk and use it as corpus\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`word_tokenize` from `nltk.tokenize`\n",
    "\n",
    "`simple_preprocess` from `gensim.utils`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Preprocess the data\n",
    "- Create a corpus\n",
    "- Optional: Load book from disk\n",
    "- Optional: Split sentences\n",
    "- Optional: Create corpus from whole book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b998d61-8a3e-46ce-9fba-c37b68bade66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde32fe",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3ed40-ae44-4c4c-8778-3a91cba8b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "sentences = [simple_preprocess(s) for s in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e961c68-893b-42c0-bf88-1bbac68ed6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "doc = open('data/guide.txt', encoding ='utf-8')\n",
    "\n",
    "sentences =[]\n",
    "for sentence in doc.read().split('.'):\n",
    "  sentences.append(simple_preprocess(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c25b8-5643-4db9-8fc0-8822a52e2879",
   "metadata": {},
   "source": [
    "## üß™ Exercise 1: Create Model and explore word similarities\n",
    "\n",
    "**Goal:** Train a Word2Vec Model \n",
    "\n",
    "**Optional:** Explore different parameters\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`Word2Vec` from `gensim.models`\n",
    "\n",
    "`wv.most_similar` from your trained model\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Create an instance of `Word2Vec`\n",
    "- Use the tokenized corpus as data\n",
    "- Test some word similarities\n",
    "- Optional: Make one with `skip-gram` and one with `CBOW`\n",
    "- OPtional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcddb9f8-0093-4456-a444-1004ecd3b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed9d43-ad0e-4c0f-b29a-96be001271d2",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d6d94-796e-4716-9b6d-06820438b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=2, sg=1)\n",
    "model.save(\"hitchhiker_word2vec_sg.model\")\n",
    "\n",
    "model.wv.most_similar(\"fish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6436b3",
   "metadata": {},
   "source": [
    "## üß™ Exercise 2: Visualize with UMAP\n",
    "\n",
    "**UMAP (Uniform Manifold Approximation and Projection)** is a dimensionality reduction technique that preserves both local and global structure in data, making it great for visualizing high-dimensional embeddings. It works by modeling the data as a graph and optimizing a low-dimensional representation that maintains the original relationships as closely as possible.\n",
    "\n",
    "**Goal:** Identify the most meaningful words in each sentence\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`UMAP` from `umap`\n",
    "\n",
    "`array` from `numpy`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Extract all words and vectors\n",
    "- Reduce dimensions\n",
    "- Plot the result\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "Plot of the reduced word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5000b53-3329-4e6c-8b5e-d94822223b89",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b2ef0-d55f-40f1-bce4-9138a739d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "words = list(model.wv.index_to_key)\n",
    "word_vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, metric='cosine', random_state=42)\n",
    "embeddings_2d = reducer.fit_transform(word_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    plt.scatter(x, y, s=10)\n",
    "    #plt.text(x + 0.1, y + 0.1, words[i], fontsize=8)\n",
    "\n",
    "plt.title(\"UMAP Projection of Embeddings\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc5c91-4cf2-40fd-852b-bd3fa761ec51",
   "metadata": {},
   "source": [
    "## üß™ Exercise 3: Interactive plot with plotly\n",
    "\n",
    "**Goal:** Create a plot where you can interactively explore the UMAP data\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`scatter` from `plotly`\n",
    "\n",
    "`dataframe` from `pandas`\n",
    "\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Prepare a dataframe\n",
    "- Make an interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb89f8-a45d-41f0-8e70-08cd38fb19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555bdb4-ab88-41dc-930b-2d02d8fc3b97",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe95ce-fd70-4f5a-bbea-d3dcde2fcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embeddings_2d, columns=[\"x\", \"y\"])\n",
    "df[\"word\"] = words\n",
    "\n",
    "fig = px.scatter(df, x=\"x\", y=\"y\", text=\"word\", title=\"Word Embeddings (Word2Vec + UMAP)\")\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7a842-8cc2-4b16-bef6-8690640aebf2",
   "metadata": {},
   "source": [
    "## üß™ Exercise 4: Use pre-trained Word2Vec\n",
    "\n",
    "**Goal:** Load a pre-trained Word2Vec model and explore what is different\n",
    "\n",
    "**Optional:** Load two pre-trained Word2Vec models and explore how they differ\n",
    "\n",
    "**Super Optional:** Visualize some word from a pretrained Word2Vec model\n",
    "\n",
    "**Super Super Optiona:** Visualize some words from two pretrained models in the same plot\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`load` from `gensim.downloader`\n",
    "\n",
    "`info` from `gensim.downloader` (Use this to list available models)\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Load a pretrained model\n",
    "- Create a list of words you want to test\n",
    "- Calculate the 3 most similar\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "üîé Word: computer\n",
    "3 most similar: [('computers', 0.916504442691803), ('software', 0.8814992904663086), ('technology', 0.852556049823761)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6824223-e923-4b87-a15d-78cbd05e7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6cce4-dc5e-43a4-9853-483ce96e2317",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d57ca-3568-4880-b302-91a84f637c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Example: Load two models\n",
    "model_glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "words_to_test = [\"king\", \"apple\", \"computer\", \"music\"]\n",
    "\n",
    "for word in words_to_test:\n",
    "    print(f\"\\nüîé Word: {word}\")\n",
    "    print(\"3 most similar:\", model_glove.most_similar(word)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbd264-228e-4db2-a5d2-826702808afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Step 1: Load pretrained models\n",
    "model_glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "model_glove_twitter = api.load(\"glove-twitter-50\")\n",
    "\n",
    "# Step 2: Select common words to compare\n",
    "words_to_plot = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"orange\", \"computer\", \"music\", \"city\", \"doctor\"]\n",
    "\n",
    "# Filter only words available in both vocabularies\n",
    "words_common = [word for word in words_to_plot if word in model_glove_twitter and word in model_glove]\n",
    "\n",
    "# Step 3: Collect vectors\n",
    "vectors = []\n",
    "labels = []\n",
    "sources = []\n",
    "\n",
    "for word in words_common:\n",
    "    vectors.append(model_glove_twitter[word])\n",
    "    labels.append(word)\n",
    "    sources.append(\"GloveTwitter\")\n",
    "\n",
    "    vectors.append(model_glove[word])\n",
    "    labels.append(word)\n",
    "    sources.append(\"GloVe\")\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "# Step 4: Reduce dimensions with UMAP\n",
    "reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, metric='cosine', random_state=42)\n",
    "embedding_2d = reducer.fit_transform(vectors)\n",
    "\n",
    "# Step 5: Prepare dataframe for plotting\n",
    "df = pd.DataFrame(embedding_2d, columns=[\"x\", \"y\"])\n",
    "df[\"word\"] = labels\n",
    "df[\"model\"] = sources\n",
    "\n",
    "# Step 6: Plot with Plotly\n",
    "fig = px.scatter(df, x=\"x\", y=\"y\", text=\"word\", color=\"model\",\n",
    "                 title=\"Comparison of Word Embeddings from Glove Twitter and GloVe Gigawords\")\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_layout(legend_title_text='Embedding Source')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065f362-f1d0-4b28-8066-b4d1ae87c332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
