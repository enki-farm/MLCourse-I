{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c01dcf",
   "metadata": {},
   "source": [
    "# üß† NLP 101 for Programmers\n",
    "\n",
    "## Featuring The Hitchhiker‚Äôs Guide to the Galaxy\n",
    "### ‚è±Ô∏è Duration: ~30 minutes\n",
    "### üõ†Ô∏è Requirements: Python 3, Jupyter Notebook or any Python IDE, nltk, scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba0057",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Overview\n",
    "\n",
    "Welcome to your first dive into NLP! In this tutorial, we‚Äôll explore how machines process and understand text. We‚Äôll start with:\n",
    "- Tokenization ‚Äì breaking down text into individual units\n",
    "- Bag of Words (BoW) ‚Äì a simple representation of text\n",
    "- TF-IDF ‚Äì identifying important words in context\n",
    "\n",
    "You'll work on short excerpts from The Hitchhiker‚Äôs Guide to the Galaxy and complete three exercises along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a2cd3",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041ec87",
   "metadata": {},
   "source": [
    "## üß™ Exercise 1: Tokenization\n",
    "\n",
    "**Goal:** Break a passage into tokens\n",
    "\n",
    "**Optional:** Preprocess it to remove punctuation, numbers and stopwords\n",
    "\n",
    "**Super Optional:** Visualize the result with a bar plot\n",
    "\n",
    "### üìñ Sample Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e080929",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies \n",
    "a small unregarded yellow sun.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dab795",
   "metadata": {},
   "source": [
    "###¬†üß∞ Tools:\n",
    "\n",
    "`word_tokenize` from `nltk.tokenize`\n",
    "`Counter` from `collections`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Tokenize the above text.\n",
    "- Count the number of unique tokens.\n",
    "- Print the top 5 most frequent tokens.\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "Tokens: ['Far', 'out', 'in', 'the', 'uncharted', 'backwaters', ...]\n",
    "Unique tokens: 19\n",
    "Most frequent: [('the', 3), ('of', 2), ('Far', 1), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b998d61-8a3e-46ce-9fba-c37b68bade66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde32fe",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37a24f-c41f-481e-9295-e7382250ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "freq_dist = Counter(tokens)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "print(f\"Most frequent: {freq_dist.most_common(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e961c68-893b-42c0-bf88-1bbac68ed6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # remove punctuation/numbers\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]  # remove stopwords\n",
    "    # TODO: lemma\n",
    "    # TODO: stem\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess(text)\n",
    "freq_dist = Counter(tokens)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "print(f\"Most frequent: {freq_dist.most_common(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text.lower())\n",
    "freq_dist = Counter(tokens)\n",
    "most_common = freq_dist.most_common(5)\n",
    "words, counts = zip(*most_common)\n",
    "sns.barplot(x=list(words), y=list(counts))\n",
    "plt.title(\"Top 5 Words in Sample Text\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1680132",
   "metadata": {},
   "source": [
    "## üß™ Exercise 2: Bag of Words\n",
    "\n",
    "**Goal:** Represent text as a word-count vector\n",
    "\n",
    "**Optional:*** Visualize the result with a heatmap\n",
    "\n",
    "### üìñ Sample Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b526a2f4-6eb3-44e8-a395-597b7ecdc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"The ships hung in the sky in much the same way that bricks don‚Äôt.\",\n",
    "    \"Time is an illusion. Lunchtime doubly so.\",\n",
    "    \"The Answer to the Great Question... Of Life, the Universe and Everything... Is... Forty-two.\",\n",
    "    \"It was a  particular  type  of  rain  he  particularly  disliked, particularly  when he was driving. He had a number for it. It was rain type 17.\",\n",
    "    \"He blinked, and understood nothing.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c25b8-5643-4db9-8fc0-8822a52e2879",
   "metadata": {},
   "source": [
    "###¬†üß∞ Tools:\n",
    "\n",
    "`CountVectorizer` from `sklearn.feature_extraction.text`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Convert the 3 texts into a Bag of Words representation.\n",
    "- Print the vocabulary.\n",
    "- Print the count matrix as a DataFrame for readability.\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "Vocabulary: ['answer', 'bricks', 'don‚Äôt', 'everything', ...]\n",
    "BoW Matrix:\n",
    "|        | answer | bricks | don‚Äôt | everything | ... |\n",
    "|--------|--------|--------|-------|------------|-----|\n",
    "| Text 1 | 0      | 1      | 1     | 0          | ... |\n",
    "| Text 2 | 0      | 0      | 0     | 0          | ... |\n",
    "| Text 3 | 1      | 0      | 0     | 1          | ... |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcddb9f8-0093-4456-a444-1004ecd3b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed9d43-ad0e-4c0f-b29a-96be001271d2",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d6d94-796e-4716-9b6d-06820438b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "bow = vectorizer.fit_transform(docs)\n",
    "\n",
    "df_bow = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Matrix:\\n\", df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_bow, annot=True, cmap=\"YlGnBu\", cbar=False)\n",
    "plt.title(\"Bag of Words Matrix\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Text Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6436b3",
   "metadata": {},
   "source": [
    "## üß™ Exercise 3: TF-IDF\n",
    "\n",
    "**Goal:** Identify the most meaningful words in each sentence\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`TfidfVectorizer` from `sklearn.feature_extraction.text`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Convert the same texts into TF-IDF vectors.\n",
    "- Print the resulting matrix as a DataFrame.\n",
    "- Highlight the top 3 words with the highest TF-IDF scores per text.\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "TF-IDF Matrix:\n",
    "|        | answer | bricks | don‚Äôt | everything | ... |\n",
    "|--------|--------|--------|-------|------------|-----|\n",
    "| Text 1 | 0.0    | 0.707  | 0.707 | 0.0        | ... |\n",
    "| Text 2 | 0.0    | 0.0    | 0.0   | 0.0        | ... |\n",
    "| Text 3 | 0.5    | 0.0    | 0.0   | 0.5        | ... |\n",
    "\n",
    "Top words:\n",
    "- Text 1: bricks, don‚Äôt, sky\n",
    "- Text 2: illusion, lunchtime, doubly\n",
    "- Text 3: answer, everything, universe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48cce64-5303-45f1-abcb-dc4ea3abc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b2ef0-d55f-40f1-bce4-9138a739d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "df_tfidf = pd.DataFrame(tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_tfidf, annot=False, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"TF-IDF Scores per Word\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Text Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fc1a2-f9b6-4811-90b4-0bbca8126b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Compute similarity/distance matrices\n",
    "cos_sim = cosine_similarity(X_tfidf)\n",
    "eucl_dist = euclidean_distances(X_tfidf)\n",
    "manh_dist = manhattan_distances(X_tfidf)\n",
    "\n",
    "# Function to plot heatmaps\n",
    "def plot_heatmap(matrix, title, labels):\n",
    "    df = pd.DataFrame(matrix, index=labels, columns=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df, annot=True, cmap=\"viridis\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "labels = [f'Doc {i+1}' for i in range(len(docs))]\n",
    "\n",
    "# Plot results\n",
    "plot_heatmap(cos_sim, \"Cosine Similarity\", labels)\n",
    "plot_heatmap(eucl_dist, \"Euclidean Distance\", labels)\n",
    "plot_heatmap(manh_dist, \"Manhattan Distance\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cada0c5-0545-43c1-9c64-c152c093ff1a",
   "metadata": {},
   "source": [
    "## üß™ Exercise 4: Word-Level Distance Comparisons\n",
    "\n",
    "**Goal:** Explore how close or far apart individual words are based on the vector space created during tokenization.\n",
    "\n",
    "**Optional:** Implement a simple k-Nearest Neighbours\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`cosine_similarity` (or any other) from `sklearn.metrics.pairwise`\n",
    "\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Select two words from the dictionary\n",
    "- Calculate the distance between them\n",
    "- Optional: Pick a word and calculate all distances to the other words (order them ascending) - print k\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "Word 1 has distance 0.457234 to Word 2\n",
    "\n",
    "# optional\n",
    "The 3 nearest words to word 1 are:\n",
    "Word 2: 0.0123\n",
    "Word 7: 0.1543\n",
    "Word 4: 0.2872\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b250a6a-f042-4eb3-a9b4-c1347f4a9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7dfa4-64f1-42c5-92ba-806a3353f2c1",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4990f41-134a-426e-97ac-76bb832f7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Get vocabulary\n",
    "vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Pick two words\n",
    "word1 = \"illusion\"\n",
    "word2 = \"rain\"\n",
    "\n",
    "# Ensure both words exist in vocab\n",
    "if word1 in vocab_dict and word2 in vocab_dict:\n",
    "    vec1 = tfidf[:, vocab_dict[word1]].toarray()\n",
    "    vec2 = tfidf[:, vocab_dict[word2]].toarray()\n",
    "    \n",
    "    eucl = euclidean_distances(vec1.T, vec2.T)[0][0]\n",
    "    \n",
    "    print(f\"Euclidean distance between '{word1}' and '{word2}': {eucl:.4f}\")\n",
    "else:\n",
    "    print(\"One of the words is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e1f5b-bc5c-41c0-af11-815870d6c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose target word\n",
    "target_word = \"rain\"\n",
    "\n",
    "# Ensure word exists\n",
    "if target_word in vocab_dict:\n",
    "    target_vec = tfidf[:, vocab_dict[target_word]].toarray()\n",
    "    \n",
    "    distances = {}\n",
    "    for word in vocab:\n",
    "        if word == target_word:\n",
    "            continue\n",
    "        word_vec = tfidf[:, vocab_dict[word]].toarray()\n",
    "        dist = euclidean_distances(target_vec.T, word_vec.T)[0][0]\n",
    "        distances[word] = dist\n",
    "\n",
    "    # Sort by closest\n",
    "    sorted_words = sorted(distances.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\nTop 5 words closest to '{target_word}' by euclidean distance:\")\n",
    "    for word, dist in sorted_words[:3]:\n",
    "        print(f\"{word}: {dist:.4f}\")\n",
    "else:\n",
    "    print(\"Target word is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc5c91-4cf2-40fd-852b-bd3fa761ec51",
   "metadata": {},
   "source": [
    "## üß™ Exercise 5: Cosine Similarity Between Texts\n",
    "\n",
    "**Goal:** Find which texts are most similar using vector math\n",
    "\n",
    "**Optional:** Test different Metrics (checkout `sklearn.metrics.pairwise`)\n",
    "\n",
    "**Super Optional:** Compare with preprocessed texts\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`cosine_similarity` from `sklearn.metrics.pairwise`\n",
    "\n",
    "`heatmap` from `seaborn`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Calculate the Cosine Similarity between all vectors\n",
    "- Print the resulting matrix as a DataFrame.\n",
    "- Create a heatmap to visualize the most similar texts\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "Heatmap plot of doc simillarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb89f8-a45d-41f0-8e70-08cd38fb19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555bdb4-ab88-41dc-930b-2d02d8fc3b97",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe95ce-fd70-4f5a-bbea-d3dcde2fcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess vergleich\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to plot heatmaps\n",
    "def plot_heatmap(matrix, title, labels):\n",
    "    df = pd.DataFrame(matrix, index=labels, columns=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df, annot=True, cmap=\"viridis\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "labels = [f'Doc {i+1}' for i in range(len(docs))]\n",
    "\n",
    "cos_sim = cosine_similarity(X_tfidf)\n",
    "plot_heatmap(cos_sim, \"Cosine Similarity\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbd264-228e-4db2-a5d2-826702808afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity/distance matrices\n",
    "eucl_dist = euclidean_distances(X_tfidf)\n",
    "manh_dist = manhattan_distances(X_tfidf)\n",
    "\n",
    "plot_heatmap(eucl_dist, \"Euclidean Distance\", labels)\n",
    "plot_heatmap(manh_dist, \"Manhattan Distance\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d57ca-3568-4880-b302-91a84f637c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Fix this!\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # remove punctuation/numbers\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]  # remove stopwords\n",
    "    # TODO: lemma\n",
    "    # TODO: stem\n",
    "    return tokens\n",
    "\n",
    "pr_docs = [preprocess(text) for text in docs]\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "similarity_matrix = cosine_similarity(tfidf)\n",
    "df_similarity = pd.DataFrame(similarity_matrix, index=[f\"Text {i+1}\" for i in range(len(docs))],\n",
    "                             columns=[f\"Text {i+1}\" for i in range(len(docs))])\n",
    "\n",
    "sns.heatmap(df_similarity, annot=True, cmap=\"Blues\")\n",
    "plt.title(\"Cosine Similarity Between Texts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b91786-f505-48e9-aa15-3145f1b0c278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
