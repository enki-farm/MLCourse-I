{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c01dcf",
   "metadata": {},
   "source": [
    "# üß† NLP 101 for Programmers\n",
    "\n",
    "## Featuring The Hitchhiker‚Äôs Guide to the Galaxy\n",
    "### ‚è±Ô∏è Duration: ~30 minutes\n",
    "### üõ†Ô∏è Requirements: Python 3, Jupyter Notebook or any Python IDE, nltk, scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba0057",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Overview\n",
    "\n",
    "Welcome to your first dive into NLP! In this tutorial, we‚Äôll explore how machines process and understand text. We‚Äôll start with:\n",
    "- Tokenization ‚Äì breaking down text into individual units\n",
    "- Bag of Words (BoW) ‚Äì a simple representation of text\n",
    "- TF-IDF ‚Äì identifying important words in context\n",
    "\n",
    "You'll work on short excerpts from The Hitchhiker‚Äôs Guide to the Galaxy and complete three exercises along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a2cd3",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041ec87",
   "metadata": {},
   "source": [
    "## üß™ Exercise 1: Tokenization\n",
    "\n",
    "**Goal:** Break a passage into tokens\n",
    "\n",
    "**Optional:** Preprocess it to remove punctuation, numbers and stopwords\n",
    "\n",
    "**Super Optional:** Visualize the result with a bar plot\n",
    "\n",
    "### üìñ Sample Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e080929",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies \n",
    "a small unregarded yellow sun.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dab795",
   "metadata": {},
   "source": [
    "###¬†üß∞ Tools:\n",
    "\n",
    "`word_tokenize` from `nltk.tokenize`\n",
    "`Counter` from `collections`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Tokenize the above text.\n",
    "- Count the number of unique tokens.\n",
    "- Print the top 5 most frequent tokens.\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "Tokens: ['Far', 'out', 'in', 'the', 'uncharted', 'backwaters', ...]\n",
    "Unique tokens: 19\n",
    "Most frequent: [('the', 3), ('of', 2), ('Far', 1), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b998d61-8a3e-46ce-9fba-c37b68bade66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde32fe",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37a24f-c41f-481e-9295-e7382250ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "freq_dist = Counter(tokens)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "print(f\"Most frequent: {freq_dist.most_common(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e961c68-893b-42c0-bf88-1bbac68ed6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # remove punctuation/numbers\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]  # remove stopwords\n",
    "    # TODO: lemma\n",
    "    # TODO: stem\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess(text)\n",
    "freq_dist = Counter(tokens)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "print(f\"Most frequent: {freq_dist.most_common(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text.lower())\n",
    "freq_dist = Counter(tokens)\n",
    "most_common = freq_dist.most_common(5)\n",
    "words, counts = zip(*most_common)\n",
    "sns.barplot(x=list(words), y=list(counts))\n",
    "plt.title(\"Top 5 Words in Sample Text\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1680132",
   "metadata": {},
   "source": [
    "## üß™ Exercise 2: Bag of Words\n",
    "\n",
    "**Goal:** Represent text as a word-count vector\n",
    "\n",
    "**Optional:*** Visualize the result with a heatmap\n",
    "\n",
    "### üìñ Sample Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b526a2f4-6eb3-44e8-a395-597b7ecdc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"The ships hung in the sky in much the same way that bricks don‚Äôt.\",\n",
    "    \"Time is an illusion. Lunchtime doubly so.\",\n",
    "    \"The Answer to the Great Question... Of Life, the Universe and Everything... Is... Forty-two.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c25b8-5643-4db9-8fc0-8822a52e2879",
   "metadata": {},
   "source": [
    "###¬†üß∞ Tools:\n",
    "\n",
    "`CountVectorizer` from `sklearn.feature_extraction.text`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Convert the 3 texts into a Bag of Words representation.\n",
    "- Print the vocabulary.\n",
    "- Print the count matrix as a DataFrame for readability.\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "Vocabulary: ['answer', 'bricks', 'don‚Äôt', 'everything', ...]\n",
    "BoW Matrix:\n",
    "|        | answer | bricks | don‚Äôt | everything | ... |\n",
    "|--------|--------|--------|-------|------------|-----|\n",
    "| Text 1 | 0      | 1      | 1     | 0          | ... |\n",
    "| Text 2 | 0      | 0      | 0     | 0          | ... |\n",
    "| Text 3 | 1      | 0      | 0     | 1          | ... |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcddb9f8-0093-4456-a444-1004ecd3b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed9d43-ad0e-4c0f-b29a-96be001271d2",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d6d94-796e-4716-9b6d-06820438b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "bow = vectorizer.fit_transform(docs)\n",
    "\n",
    "df_bow = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Matrix:\\n\", df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_bow, annot=True, cmap=\"YlGnBu\", cbar=False)\n",
    "plt.title(\"Bag of Words Matrix\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Text Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6436b3",
   "metadata": {},
   "source": [
    "## üß™ Exercise 3: TF-IDF\n",
    "\n",
    "**Goal:** Identify the most meaningful words in each sentence\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`TfidfVectorizer` from `sklearn.feature_extraction.text`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Convert the same texts into TF-IDF vectors.\n",
    "- Print the resulting matrix as a DataFrame.\n",
    "- Highlight the top 3 words with the highest TF-IDF scores per text.\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "TF-IDF Matrix:\n",
    "|        | answer | bricks | don‚Äôt | everything | ... |\n",
    "|--------|--------|--------|-------|------------|-----|\n",
    "| Text 1 | 0.0    | 0.707  | 0.707 | 0.0        | ... |\n",
    "| Text 2 | 0.0    | 0.0    | 0.0   | 0.0        | ... |\n",
    "| Text 3 | 0.5    | 0.0    | 0.0   | 0.5        | ... |\n",
    "\n",
    "Top words:\n",
    "- Text 1: bricks, don‚Äôt, sky\n",
    "- Text 2: illusion, lunchtime, doubly\n",
    "- Text 3: answer, everything, universe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48cce64-5303-45f1-abcb-dc4ea3abc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b2ef0-d55f-40f1-bce4-9138a739d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "df_tfidf = pd.DataFrame(tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_tfidf, annot=False, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"TF-IDF Scores per Word\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Text Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc5c91-4cf2-40fd-852b-bd3fa761ec51",
   "metadata": {},
   "source": [
    "## üß™ Exercise 4: Cosine Similarity Between Texts\n",
    "\n",
    "**Goal:** Find which texts are most similar using vector math\n",
    "\n",
    "**Optional:** Compare with preprocessed texts\n",
    "\n",
    "###¬†üß∞ Tools:\n",
    "\n",
    "`cosine_similarity` from `sklearn.metrics.pairwise`\n",
    "\n",
    "`heatmap` from `seaborn`\n",
    "\n",
    "###¬†üíª Task:\n",
    "- Calculate the Cosine Similarity between all vectors\n",
    "- Print the resulting matrix as a DataFrame.\n",
    "- Create a heatmap to visualize the most similar texts\n",
    "\n",
    "###¬†‚úÖ Expected Output (example):\n",
    "\n",
    "```python\n",
    "Text 1 is most similar to Text 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb89f8-a45d-41f0-8e70-08cd38fb19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555bdb4-ab88-41dc-930b-2d02d8fc3b97",
   "metadata": {},
   "source": [
    "### üìñ Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe95ce-fd70-4f5a-bbea-d3dcde2fcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess vergleich\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tfidf)\n",
    "df_similarity = pd.DataFrame(similarity_matrix, index=[f\"Text {i+1}\" for i in range(len(docs))],\n",
    "                             columns=[f\"Text {i+1}\" for i in range(len(docs))])\n",
    "\n",
    "sns.heatmap(df_similarity, annot=True, cmap=\"Blues\")\n",
    "plt.title(\"Cosine Similarity Between Texts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924507e-346d-46da-88cc-461c1a05d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"The ships hung in the sky in much the same way that bricks don‚Äôt.\",\n",
    "    \"Time is an illusion. Lunchtime doubly so.\",\n",
    "    \"The Answer to the Great Question... Of Life, the Universe and Everything... Is... Forty-two.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696d881-a028-47d0-8879-c5bfca1aabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d57ca-3568-4880-b302-91a84f637c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    print(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # remove punctuation/numbers\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]  # remove stopwords\n",
    "    # TODO: lemma\n",
    "    # TODO: stem\n",
    "    return tokens\n",
    "\n",
    "docs = [preprocess(text) for text in docs]\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(pr)\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "similarity_matrix = cosine_similarity(tfidf)\n",
    "df_similarity = pd.DataFrame(similarity_matrix, index=[f\"Text {i+1}\" for i in range(len(docs))],\n",
    "                             columns=[f\"Text {i+1}\" for i in range(len(docs))])\n",
    "\n",
    "sns.heatmap(df_similarity, annot=True, cmap=\"Blues\")\n",
    "plt.title(\"Cosine Similarity Between Texts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbd264-228e-4db2-a5d2-826702808afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
